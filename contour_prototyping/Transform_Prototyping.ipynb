{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import librosa as lr\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "import spleeter\n",
    "from spleeter.separator import Separator\n",
    "from spleeter.audio.adapter import get_default_audio_adapter\n",
    "\n",
    "import crepe\n",
    "\n",
    "import mir_eval\n",
    "from mir_eval import melody\n",
    "\n",
    "import jams\n",
    "\n",
    "EXAMPLE_AUDIO_PATH = \"./Excerpt.3.15b.wav\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Apply unet for vocals_spectrogram\n",
      "WARNING:tensorflow:From /scratch/ci411/.conda/envs/rap_env/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "INFO:tensorflow:Apply unet for accompaniment_spectrogram\n",
      "WARNING:tensorflow:From /scratch/ci411/.conda/envs/rap_env/lib/python3.6/site-packages/spleeter/separator.py:158: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:spleeter:Downloading model archive https://github.com/deezer/spleeter/releases/download/v1.4.0/2stems.tar.gz\n",
      "INFO:spleeter:Validating archive checksum\n",
      "INFO:spleeter:Extracting downloaded 2stems archive\n",
      "INFO:spleeter:2stems model file(s) extracted\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /scratch/ci411/.conda/envs/rap_env/lib/python3.6/site-packages/spleeter/separator.py:160: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "INFO:tensorflow:Restoring parameters from pretrained_models/2stems/model\n"
     ]
    }
   ],
   "source": [
    "#spleeter extraction w/ 2stems\n",
    "\n",
    "#load audio with built-in audio loader\n",
    "audio_loader = get_default_audio_adapter()\n",
    "x_t, sr = audio_loader.load(EXAMPLE_AUDIO_PATH)\n",
    "sr=int(sr)\n",
    "\n",
    "#call a separator for vox/accompaniment and extract vox\n",
    "separator = Separator('spleeter:2stems') #default for vox+accomp\n",
    "prediction_test = separator.separate(x_t)\n",
    "vox_t = lr.to_mono(prediction_test['vocals'].T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract pyin_curves\n",
    "frame_length = 2048\n",
    "hop_length = frame_length//4\n",
    "pyin_f0, pyin_vox_flag, pyin_vox_prob = lr.pyin(vox_t, lr.note_to_hz('C2'), lr.note_to_hz('C7'),\\\n",
    "                                                sr=sr, frame_length=frame_length, hop_length=hop_length)\n",
    "pyin_freq, pyin_voc = melody.freq_to_voicing(pyin_f0, voicing=pyin_vox_flag)\n",
    "pyin_time = melody.constant_hop_timebase(hop_length, hop_length*(len(pyin_f0)-1))/sr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /scratch/ci411/.conda/envs/rap_env/lib/python3.6/site-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "2101/2101 [==============================] - 82s 39ms/sample\n"
     ]
    }
   ],
   "source": [
    "#extract crepe predictions\n",
    "crepe_time, frequency, confidence, activation = crepe.predict(vox_t, sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.5\n",
    "crepe_vox_flag = confidence>threshold\n",
    "crepe_freq, crepe_voc = melody.freq_to_voicing(frequency, voicing=crepe_vox_flag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_unvoiced(time, frequency, voicings):\n",
    "    voiced_idx = np.where(voicings)\n",
    "    return time[voiced_idx], frequency[voiced_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Voicing Recall': 0.8281838733986435, 'Voicing False Alarm': 0, 'Raw Pitch Accuracy': 0.9035418236623964, 'Raw Chroma Accuracy': 0.9035418236623964, 'Overall Accuracy': 0.7822155237377544}\n",
      "{'Voicing Recall': 0.8892857142857142, 'Voicing False Alarm': 0, 'Raw Pitch Accuracy': 0.8371428571428572, 'Raw Chroma Accuracy': 0.8371428571428572, 'Overall Accuracy': 0.8371428571428572}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/ci411/.conda/envs/rap_env/lib/python3.6/site-packages/mir_eval/melody.py:569: RuntimeWarning: invalid value encountered in less\n",
      "  correct_frequencies = freq_diff_cents < cent_tolerance\n",
      "/scratch/ci411/.conda/envs/rap_env/lib/python3.6/site-packages/mir_eval/melody.py:637: RuntimeWarning: invalid value encountered in less\n",
      "  correct_chroma = np.abs(freq_diff_cents - octave) < cent_tolerance\n",
      "/scratch/ci411/.conda/envs/rap_env/lib/python3.6/site-packages/mir_eval/melody.py:697: RuntimeWarning: invalid value encountered in less\n",
      "  correct_frequencies = freq_diff_cents < cent_tolerance\n"
     ]
    }
   ],
   "source": [
    "#compute comparison metrics\n",
    "\n",
    "#pyin as reference\n",
    "pyin_ref_time, pyin_ref_freq = drop_unvoiced(pyin_time, pyin_freq, pyin_voc)\n",
    "pyin_ref_results = melody.evaluate(pyin_ref_time, melody.hz2cents(pyin_ref_freq), crepe_time, melody.hz2cents(crepe_freq), est_voicing=crepe_voc)\n",
    "pyin_ref_results = dict(pyin_ref_results)\n",
    "print(pyin_ref_results)\n",
    "\n",
    "#crepe as reference\n",
    "crepe_ref_time, crepe_ref_freq = drop_unvoiced(crepe_time, crepe_freq, crepe_voc)\n",
    "crepe_ref_results = melody.evaluate(crepe_ref_time, melody.hz2cents(crepe_ref_freq), pyin_time, melody.hz2cents(pyin_freq), est_voicing=pyin_voc)\n",
    "crepe_ref_results = dict(crepe_ref_results)\n",
    "print(crepe_ref_results)\n",
    "\n",
    "all_ref_results = {'pyin_ref_metrics':pyin_ref_results, 'crepe_ref_metrics':crepe_ref_results}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "jam = jams.JAMS()\n",
    "jam.sandbox = all_ref_results\n",
    "\n",
    "track_duration = lr.get_duration(y=x_t, sr=sr)\n",
    "jam.file_metadata.duration = track_duration\n",
    "\n",
    "pitch_ann = jams.Annotation(namespace='pitch_contour')\n",
    "\n",
    "for i in range(len(pyin_freq)):\n",
    "    value = {'index':0, 'frequency': pyin_freq[i], 'voiced':bool(pyin_voc[i])} \n",
    "    pitch_ann.append(time=pyin_time[i], value=value, duration=0, confidence=pyin_vox_prob[i])\n",
    "    \n",
    "for i in range(len(crepe_freq)):\n",
    "    value = {'index':1, 'frequency': crepe_freq[i], 'voiced':bool(crepe_voc[i])} \n",
    "    pitch_ann.append(time=crepe_time[i], value=value, duration=0, confidence=confidence[i])\n",
    "    \n",
    "jam.annotations.append(pitch_ann)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "jam.save('test_countours.jamz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_unvoiced(time, frequency, voicings):\n",
    "    voiced_idx = np.where(voicings)\n",
    "    return time[voiced_idx], frequency[voiced_idx]\n",
    "\n",
    "def process_contours(inpath, outpath, separator=Separator('spleeter:2stems')):\n",
    "    #load audio and separate vox\n",
    "    audio_loader = get_default_audio_adapter()\n",
    "    x_t, sr = audio_loader.load(inpath)\n",
    "    sr=int(sr)\n",
    "\n",
    "    #use separator for vox/accompaniment and extract vox\n",
    "    prediction_test = separator.separate(x_t)\n",
    "    vox_t = lr.to_mono(prediction_test['vocals'].T)\n",
    "    \n",
    "    #extract pyin_curves\n",
    "    frame_length = 2048\n",
    "    hop_length = frame_length//4\n",
    "    pyin_f0, pyin_vox_flag, pyin_vox_prob = lr.pyin(vox_t, lr.note_to_hz('C2'), lr.note_to_hz('C7'),\\\n",
    "                                                    sr=sr, frame_length=frame_length, hop_length=hop_length)\n",
    "    pyin_freq, pyin_voc = melody.freq_to_voicing(pyin_f0, voicing=pyin_vox_flag)\n",
    "    pyin_time = melody.constant_hop_timebase(hop_length, hop_length*(len(pyin_f0)-1))/sr\n",
    "    \n",
    "    #extract crepe predictions\n",
    "    crepe_time, frequency, confidence, activation = crepe.predict(vox_t, sr, verbose=0)\n",
    "    threshold = 0.5\n",
    "    crepe_vox_flag = confidence>threshold\n",
    "    crepe_freq, crepe_voc = melody.freq_to_voicing(frequency, voicing=crepe_vox_flag)\n",
    "    \n",
    "    #compute comparison metrics\n",
    "\n",
    "   #pyin as reference\n",
    "    pyin_ref_time, pyin_ref_freq = drop_unvoiced(pyin_time, pyin_freq, pyin_voc)\n",
    "    if len(pyin_ref_time>0):\n",
    "        pyin_ref_results = melody.evaluate(pyin_ref_time, melody.hz2cents(pyin_ref_freq), crepe_time, melody.hz2cents(crepe_freq), est_voicing=crepe_voc)\n",
    "        pyin_ref_results = dict(pyin_ref_results)\n",
    "        print(pyin_ref_results)\n",
    "    else:\n",
    "        print(\"No PYIN voicings\")\n",
    "        pyin_ref_results = float('NaN')\n",
    "\n",
    "    #crepe as reference\n",
    "    crepe_ref_time, crepe_ref_freq = drop_unvoiced(crepe_time, crepe_freq, crepe_voc)\n",
    "    if len(crepe_ref_time)>0:    \n",
    "        crepe_ref_results = melody.evaluate(crepe_ref_time, melody.hz2cents(crepe_ref_freq), pyin_time, melody.hz2cents(pyin_freq), est_voicing=pyin_voc)\n",
    "        crepe_ref_results = dict(crepe_ref_results)\n",
    "        print(crepe_ref_results)\n",
    "    else:\n",
    "        print(\"No CREPE voicings\")\n",
    "        crepe_ref_results = float('NaN')\n",
    "\n",
    "    all_ref_results = {'pyin_ref_metrics':pyin_ref_results, 'crepe_ref_metrics':crepe_ref_results}\n",
    "    \n",
    "    #save results to jams\n",
    "    jam = jams.JAMS()\n",
    "    jam.sandbox = all_ref_results\n",
    "\n",
    "    track_duration = lr.get_duration(y=x_t, sr=sr)\n",
    "    jam.file_metadata.duration = track_duration\n",
    "\n",
    "    pitch_ann = jams.Annotation(namespace='pitch_contour')\n",
    "\n",
    "    for i in range(len(pyin_freq)):\n",
    "        value = {'index':0, 'frequency': pyin_freq[i], 'voiced':bool(pyin_voc[i])} \n",
    "        pitch_ann.append(time=pyin_time[i], value=value, duration=0, confidence=pyin_vox_prob[i])\n",
    "\n",
    "    for i in range(len(crepe_freq)):\n",
    "        value = {'index':1, 'frequency': crepe_freq[i], 'voiced':bool(crepe_voc[i])} \n",
    "        pitch_ann.append(time=crepe_time[i], value=value, duration=0, confidence=confidence[i])\n",
    "\n",
    "    jam.annotations.append(pitch_ann)\n",
    "    jam.save(outpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Apply unet for vocals_spectrogram\n",
      "INFO:tensorflow:Apply unet for accompaniment_spectrogram\n",
      "INFO:tensorflow:Restoring parameters from pretrained_models/2stems/model\n",
      "{'Voicing Recall': 0.8281838733986435, 'Voicing False Alarm': 0, 'Raw Pitch Accuracy': 0.9035418236623964, 'Raw Chroma Accuracy': 0.9035418236623964, 'Overall Accuracy': 0.7822155237377544}\n",
      "{'Voicing Recall': 0.8892857142857142, 'Voicing False Alarm': 0, 'Raw Pitch Accuracy': 0.8371428571428572, 'Raw Chroma Accuracy': 0.8371428571428572, 'Overall Accuracy': 0.8371428571428572}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/ci411/.conda/envs/rap_env/lib/python3.6/site-packages/mir_eval/melody.py:569: RuntimeWarning: invalid value encountered in less\n",
      "  correct_frequencies = freq_diff_cents < cent_tolerance\n",
      "/scratch/ci411/.conda/envs/rap_env/lib/python3.6/site-packages/mir_eval/melody.py:637: RuntimeWarning: invalid value encountered in less\n",
      "  correct_chroma = np.abs(freq_diff_cents - octave) < cent_tolerance\n",
      "/scratch/ci411/.conda/envs/rap_env/lib/python3.6/site-packages/mir_eval/melody.py:697: RuntimeWarning: invalid value encountered in less\n",
      "  correct_frequencies = freq_diff_cents < cent_tolerance\n"
     ]
    }
   ],
   "source": [
    "separator = Separator('spleeter:2stems')\n",
    "out_path_test = 'test_countours_fun.jams'\n",
    "process_countours(EXAMPLE_AUDIO_PATH, out_path_test, separator=separator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "jams_load = jams.load(out_path_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/62 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "000853.mp3\n",
      "{'Voicing Recall': 0.7556608741442865, 'Voicing False Alarm': 0, 'Raw Pitch Accuracy': 0.7198525539757767, 'Raw Chroma Accuracy': 0.7198525539757767, 'Overall Accuracy': 0.6034755134281201}\n",
      "{'Voicing Recall': 0.8293293293293293, 'Voicing False Alarm': 0, 'Raw Pitch Accuracy': 0.6566566566566566, 'Raw Chroma Accuracy': 0.6566566566566566, 'Overall Accuracy': 0.6566566566566566}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/ci411/.conda/envs/rap_env/lib/python3.6/site-packages/mir_eval/melody.py:569: RuntimeWarning: invalid value encountered in less\n",
      "  correct_frequencies = freq_diff_cents < cent_tolerance\n",
      "/scratch/ci411/.conda/envs/rap_env/lib/python3.6/site-packages/mir_eval/melody.py:637: RuntimeWarning: invalid value encountered in less\n",
      "  correct_chroma = np.abs(freq_diff_cents - octave) < cent_tolerance\n",
      "/scratch/ci411/.conda/envs/rap_env/lib/python3.6/site-packages/mir_eval/melody.py:697: RuntimeWarning: invalid value encountered in less\n",
      "  correct_frequencies = freq_diff_cents < cent_tolerance\n",
      "  2%|▏         | 1/62 [01:04<1:05:46, 64.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "000200.mp3\n",
      "{'Voicing Recall': 0.8811349693251533, 'Voicing False Alarm': 0, 'Raw Pitch Accuracy': 0.8619631901840491, 'Raw Chroma Accuracy': 0.8619631901840491, 'Overall Accuracy': 0.7883435582822086}\n",
      "{'Voicing Recall': 0.7793594306049823, 'Voicing False Alarm': 0, 'Raw Pitch Accuracy': 0.697508896797153, 'Raw Chroma Accuracy': 0.697508896797153, 'Overall Accuracy': 0.697508896797153}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/ci411/.conda/envs/rap_env/lib/python3.6/site-packages/mir_eval/melody.py:569: RuntimeWarning: invalid value encountered in less\n",
      "  correct_frequencies = freq_diff_cents < cent_tolerance\n",
      "/scratch/ci411/.conda/envs/rap_env/lib/python3.6/site-packages/mir_eval/melody.py:637: RuntimeWarning: invalid value encountered in less\n",
      "  correct_chroma = np.abs(freq_diff_cents - octave) < cent_tolerance\n",
      "/scratch/ci411/.conda/envs/rap_env/lib/python3.6/site-packages/mir_eval/melody.py:697: RuntimeWarning: invalid value encountered in less\n",
      "  correct_frequencies = freq_diff_cents < cent_tolerance\n",
      "  3%|▎         | 2/62 [02:09<1:04:28, 64.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "000193.mp3\n",
      "{'Voicing Recall': 0.876750700280112, 'Voicing False Alarm': 0, 'Raw Pitch Accuracy': 0.9467787114845938, 'Raw Chroma Accuracy': 0.9467787114845938, 'Overall Accuracy': 0.8515406162464986}\n",
      "{'Voicing Recall': 0.40795454545454546, 'Voicing False Alarm': 0, 'Raw Pitch Accuracy': 0.39545454545454545, 'Raw Chroma Accuracy': 0.39545454545454545, 'Overall Accuracy': 0.39545454545454545}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/ci411/.conda/envs/rap_env/lib/python3.6/site-packages/mir_eval/melody.py:569: RuntimeWarning: invalid value encountered in less\n",
      "  correct_frequencies = freq_diff_cents < cent_tolerance\n",
      "/scratch/ci411/.conda/envs/rap_env/lib/python3.6/site-packages/mir_eval/melody.py:637: RuntimeWarning: invalid value encountered in less\n",
      "  correct_chroma = np.abs(freq_diff_cents - octave) < cent_tolerance\n",
      "/scratch/ci411/.conda/envs/rap_env/lib/python3.6/site-packages/mir_eval/melody.py:697: RuntimeWarning: invalid value encountered in less\n",
      "  correct_frequencies = freq_diff_cents < cent_tolerance\n",
      "  5%|▍         | 3/62 [03:12<1:03:04, 64.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "000002.mp3\n",
      "{'Voicing Recall': 0.7627281460134486, 'Voicing False Alarm': 0, 'Raw Pitch Accuracy': 0.7550432276657061, 'Raw Chroma Accuracy': 0.7550432276657061, 'Overall Accuracy': 0.6340057636887608}\n",
      "{'Voicing Recall': 0.5511713933415536, 'Voicing False Alarm': 0, 'Raw Pitch Accuracy': 0.46177558569667077, 'Raw Chroma Accuracy': 0.46177558569667077, 'Overall Accuracy': 0.46177558569667077}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/ci411/.conda/envs/rap_env/lib/python3.6/site-packages/mir_eval/melody.py:569: RuntimeWarning: invalid value encountered in less\n",
      "  correct_frequencies = freq_diff_cents < cent_tolerance\n",
      "/scratch/ci411/.conda/envs/rap_env/lib/python3.6/site-packages/mir_eval/melody.py:637: RuntimeWarning: invalid value encountered in less\n",
      "  correct_chroma = np.abs(freq_diff_cents - octave) < cent_tolerance\n",
      "/scratch/ci411/.conda/envs/rap_env/lib/python3.6/site-packages/mir_eval/melody.py:697: RuntimeWarning: invalid value encountered in less\n",
      "  correct_frequencies = freq_diff_cents < cent_tolerance\n",
      "  6%|▋         | 4/62 [04:17<1:02:14, 64.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "000707.mp3\n",
      "{'Voicing Recall': 0.296373779637378, 'Voicing False Alarm': 0, 'Raw Pitch Accuracy': 0.5711297071129707, 'Raw Chroma Accuracy': 0.5711297071129707, 'Overall Accuracy': 0.27266387726638774}\n",
      "{'Voicing Recall': 0.5719257540603249, 'Voicing False Alarm': 0, 'Raw Pitch Accuracy': 0.5150812064965197, 'Raw Chroma Accuracy': 0.5150812064965197, 'Overall Accuracy': 0.5150812064965197}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/ci411/.conda/envs/rap_env/lib/python3.6/site-packages/mir_eval/melody.py:569: RuntimeWarning: invalid value encountered in less\n",
      "  correct_frequencies = freq_diff_cents < cent_tolerance\n",
      "/scratch/ci411/.conda/envs/rap_env/lib/python3.6/site-packages/mir_eval/melody.py:637: RuntimeWarning: invalid value encountered in less\n",
      "  correct_chroma = np.abs(freq_diff_cents - octave) < cent_tolerance\n",
      "/scratch/ci411/.conda/envs/rap_env/lib/python3.6/site-packages/mir_eval/melody.py:697: RuntimeWarning: invalid value encountered in less\n",
      "  correct_frequencies = freq_diff_cents < cent_tolerance\n",
      "  8%|▊         | 5/62 [05:22<1:01:15, 64.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "000203.mp3\n",
      "{'Voicing Recall': 0.7026476578411406, 'Voicing False Alarm': 0, 'Raw Pitch Accuracy': 0.8533604887983707, 'Raw Chroma Accuracy': 0.8574338085539714, 'Overall Accuracy': 0.6619144602851323}\n",
      "{'Voicing Recall': 0.6021341463414634, 'Voicing False Alarm': 0, 'Raw Pitch Accuracy': 0.5579268292682927, 'Raw Chroma Accuracy': 0.5579268292682927, 'Overall Accuracy': 0.5579268292682927}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/ci411/.conda/envs/rap_env/lib/python3.6/site-packages/mir_eval/melody.py:569: RuntimeWarning: invalid value encountered in less\n",
      "  correct_frequencies = freq_diff_cents < cent_tolerance\n",
      "/scratch/ci411/.conda/envs/rap_env/lib/python3.6/site-packages/mir_eval/melody.py:637: RuntimeWarning: invalid value encountered in less\n",
      "  correct_chroma = np.abs(freq_diff_cents - octave) < cent_tolerance\n",
      "/scratch/ci411/.conda/envs/rap_env/lib/python3.6/site-packages/mir_eval/melody.py:697: RuntimeWarning: invalid value encountered in less\n",
      "  correct_frequencies = freq_diff_cents < cent_tolerance\n",
      " 10%|▉         | 6/62 [06:26<1:00:00, 64.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "000709.mp3\n",
      "{'Voicing Recall': 0.865902293120638, 'Voicing False Alarm': 0, 'Raw Pitch Accuracy': 0.7956131605184447, 'Raw Chroma Accuracy': 0.7956131605184447, 'Overall Accuracy': 0.734297108673978}\n",
      "{'Voicing Recall': 0.8546312178387651, 'Voicing False Alarm': 0, 'Raw Pitch Accuracy': 0.7242710120068611, 'Raw Chroma Accuracy': 0.7242710120068611, 'Overall Accuracy': 0.7242710120068611}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/ci411/.conda/envs/rap_env/lib/python3.6/site-packages/mir_eval/melody.py:569: RuntimeWarning: invalid value encountered in less\n",
      "  correct_frequencies = freq_diff_cents < cent_tolerance\n",
      "/scratch/ci411/.conda/envs/rap_env/lib/python3.6/site-packages/mir_eval/melody.py:637: RuntimeWarning: invalid value encountered in less\n",
      "  correct_chroma = np.abs(freq_diff_cents - octave) < cent_tolerance\n",
      "/scratch/ci411/.conda/envs/rap_env/lib/python3.6/site-packages/mir_eval/melody.py:697: RuntimeWarning: invalid value encountered in less\n",
      "  correct_frequencies = freq_diff_cents < cent_tolerance\n",
      " 11%|█▏        | 7/62 [07:30<58:59, 64.36s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "000690.mp3\n",
      "{'Voicing Recall': 0.38391502276176026, 'Voicing False Alarm': 0, 'Raw Pitch Accuracy': 0.6122913505311077, 'Raw Chroma Accuracy': 0.6130500758725341, 'Overall Accuracy': 0.30500758725341426}\n",
      "{'Voicing Recall': 0.7073170731707317, 'Voicing False Alarm': 0, 'Raw Pitch Accuracy': 0.5658536585365853, 'Raw Chroma Accuracy': 0.5658536585365853, 'Overall Accuracy': 0.5658536585365853}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/ci411/.conda/envs/rap_env/lib/python3.6/site-packages/mir_eval/melody.py:569: RuntimeWarning: invalid value encountered in less\n",
      "  correct_frequencies = freq_diff_cents < cent_tolerance\n",
      "/scratch/ci411/.conda/envs/rap_env/lib/python3.6/site-packages/mir_eval/melody.py:637: RuntimeWarning: invalid value encountered in less\n",
      "  correct_chroma = np.abs(freq_diff_cents - octave) < cent_tolerance\n",
      "/scratch/ci411/.conda/envs/rap_env/lib/python3.6/site-packages/mir_eval/melody.py:697: RuntimeWarning: invalid value encountered in less\n",
      "  correct_frequencies = freq_diff_cents < cent_tolerance\n",
      " 13%|█▎        | 8/62 [08:35<58:01, 64.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "000197.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 8/62 [09:38<1:05:04, 72.30s/it]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 0 is out of bounds for axis 0 with size 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-3fbd01fd7894>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0msongname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0moutpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msongname\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'.jamz'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mprocess_countours\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseparator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseparator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-10-2cfdab56afa1>\u001b[0m in \u001b[0;36mprocess_countours\u001b[0;34m(inpath, outpath, separator)\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;31m#pyin as reference\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0mpyin_ref_time\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpyin_ref_freq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdrop_unvoiced\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpyin_time\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpyin_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpyin_voc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0mpyin_ref_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmelody\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpyin_ref_time\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmelody\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhz2cents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpyin_ref_freq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcrepe_time\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmelody\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhz2cents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcrepe_freq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mest_voicing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcrepe_voc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m     \u001b[0mpyin_ref_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpyin_ref_results\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpyin_ref_results\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/scratch/ci411/.conda/envs/rap_env/lib/python3.6/site-packages/mir_eval/melody.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(ref_time, ref_freq, est_time, est_freq, est_voicing, ref_reward, **kwargs)\u001b[0m\n\u001b[1;32m    780\u001b[0m      \u001b[0mest_voicing\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mest_cent\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    781\u001b[0m          \u001b[0mto_cent_voicing\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mref_time\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mref_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mest_time\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mest_freq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 782\u001b[0;31m          est_voicing, ref_reward, **kwargs)\n\u001b[0m\u001b[1;32m    783\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    784\u001b[0m     \u001b[0;31m# Compute metrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/scratch/ci411/.conda/envs/rap_env/lib/python3.6/site-packages/mir_eval/util.py\u001b[0m in \u001b[0;36mfilter_kwargs\u001b[0;34m(_function, *args, **kwargs)\u001b[0m\n\u001b[1;32m    902\u001b[0m             \u001b[0mfiltered_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkwarg\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    903\u001b[0m     \u001b[0;31m# Call the function with the supplied args and the filtered kwarg dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 904\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfiltered_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    905\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    906\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/scratch/ci411/.conda/envs/rap_env/lib/python3.6/site-packages/mir_eval/melody.py\u001b[0m in \u001b[0;36mto_cent_voicing\u001b[0;34m(ref_time, ref_freq, est_time, est_freq, est_voicing, ref_reward, base_frequency, hop, kind)\u001b[0m\n\u001b[1;32m    358\u001b[0m     \"\"\"\n\u001b[1;32m    359\u001b[0m     \u001b[0;31m# Check if missing sample at time 0 and if so add one\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 360\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mref_time\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    361\u001b[0m         \u001b[0mref_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mref_time\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m         \u001b[0mref_freq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mref_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mref_freq\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 0 is out of bounds for axis 0 with size 0"
     ]
    }
   ],
   "source": [
    "audio_path = '/scratch/work/sonyc/marl/private_datasets/FMA/fma_small/fma_small/000'\n",
    "output_path = '/scratch/ci411/rap_data/jams_test/000'\n",
    "\n",
    "for path, subdirs, files in os.walk(datapath):\n",
    "    for file in tqdm(files):\n",
    "        print(file)\n",
    "        inpath = os.path.join(audio_path, file)\n",
    "        songname = file.split('.')[0]\n",
    "        outpath = os.path.join(output_path, songname+'.jamz')\n",
    "        process_contours(inpath, outpath, separator=separator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No PYIN voicings\n",
      "{'Voicing Recall': 0.0, 'Voicing False Alarm': 0, 'Raw Pitch Accuracy': 0.0, 'Raw Chroma Accuracy': 0.0, 'Overall Accuracy': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/ci411/.conda/envs/rap_env/lib/python3.6/site-packages/mir_eval/melody.py:91: UserWarning: Estimated melody has no voiced frames.\n",
      "  warnings.warn(\"Estimated melody has no voiced frames.\")\n",
      "/scratch/ci411/.conda/envs/rap_env/lib/python3.6/site-packages/mir_eval/melody.py:569: RuntimeWarning: invalid value encountered in less\n",
      "  correct_frequencies = freq_diff_cents < cent_tolerance\n",
      "/scratch/ci411/.conda/envs/rap_env/lib/python3.6/site-packages/mir_eval/melody.py:637: RuntimeWarning: invalid value encountered in less\n",
      "  correct_chroma = np.abs(freq_diff_cents - octave) < cent_tolerance\n",
      "/scratch/ci411/.conda/envs/rap_env/lib/python3.6/site-packages/mir_eval/melody.py:697: RuntimeWarning: invalid value encountered in less\n",
      "  correct_frequencies = freq_diff_cents < cent_tolerance\n"
     ]
    }
   ],
   "source": [
    "inpath = '/scratch/work/sonyc/marl/private_datasets/FMA/fma_small/fma_small/000/000197.mp3'\n",
    "\n",
    "#load audio and separate vox\n",
    "audio_loader = get_default_audio_adapter()\n",
    "x_t, sr = audio_loader.load(inpath)\n",
    "sr=int(sr)\n",
    "\n",
    "#use separator for vox/accompaniment and extract vox\n",
    "prediction_test = separator.separate(x_t)\n",
    "vox_t = lr.to_mono(prediction_test['vocals'].T)\n",
    "\n",
    "#extract pyin_curves\n",
    "frame_length = 2048\n",
    "hop_length = frame_length//4\n",
    "pyin_f0, pyin_vox_flag, pyin_vox_prob = lr.pyin(vox_t, lr.note_to_hz('C2'), lr.note_to_hz('C7'),\\\n",
    "                                                sr=sr, frame_length=frame_length, hop_length=hop_length)\n",
    "pyin_freq, pyin_voc = melody.freq_to_voicing(pyin_f0, voicing=pyin_vox_flag)\n",
    "pyin_time = melody.constant_hop_timebase(hop_length, hop_length*(len(pyin_f0)-1))/sr\n",
    "\n",
    "#extract crepe predictions\n",
    "crepe_time, frequency, confidence, activation = crepe.predict(vox_t, sr, verbose=0)\n",
    "threshold = 0.5\n",
    "crepe_vox_flag = confidence>threshold\n",
    "crepe_freq, crepe_voc = melody.freq_to_voicing(frequency, voicing=crepe_vox_flag)\n",
    "\n",
    "#compute comparison metrics\n",
    "\n",
    "#pyin as reference\n",
    "pyin_ref_time, pyin_ref_freq = drop_unvoiced(pyin_time, pyin_freq, pyin_voc)\n",
    "if len(pyin_ref_time>0):\n",
    "    pyin_ref_results = melody.evaluate(pyin_ref_time, melody.hz2cents(pyin_ref_freq), crepe_time, melody.hz2cents(crepe_freq), est_voicing=crepe_voc)\n",
    "    pyin_ref_results = dict(pyin_ref_results)\n",
    "    print(pyin_ref_results)\n",
    "else:\n",
    "    print(\"No PYIN voicings\")\n",
    "    pyin_ref_results = float('NaN')\n",
    "\n",
    "#crepe as reference\n",
    "crepe_ref_time, crepe_ref_freq = drop_unvoiced(crepe_time, crepe_freq, crepe_voc)\n",
    "if len(crepe_ref_time)>0:    \n",
    "    crepe_ref_results = melody.evaluate(crepe_ref_time, melody.hz2cents(crepe_ref_freq), pyin_time, melody.hz2cents(pyin_freq), est_voicing=pyin_voc)\n",
    "    crepe_ref_results = dict(crepe_ref_results)\n",
    "    print(crepe_ref_results)\n",
    "else:\n",
    "    print(\"No CREPE voicings\")\n",
    "    crepe_ref_results = float('NaN')\n",
    "\n",
    "all_ref_results = {'pyin_ref_metrics':pyin_ref_results, 'crepe_ref_metrics':crepe_ref_results}\n",
    "\n",
    "#save results to jams\n",
    "jam = jams.JAMS()\n",
    "jam.sandbox = all_ref_results\n",
    "\n",
    "track_duration = lr.get_duration(y=x_t, sr=sr)\n",
    "jam.file_metadata.duration = track_duration\n",
    "\n",
    "pitch_ann = jams.Annotation(namespace='pitch_contour')\n",
    "\n",
    "for i in range(len(pyin_freq)):\n",
    "    value = {'index':0, 'frequency': pyin_freq[i], 'voiced':bool(pyin_voc[i])} \n",
    "    pitch_ann.append(time=pyin_time[i], value=value, duration=0, confidence=pyin_vox_prob[i])\n",
    "\n",
    "for i in range(len(crepe_freq)):\n",
    "    value = {'index':1, 'frequency': crepe_freq[i], 'voiced':bool(crepe_voc[i])} \n",
    "    pitch_ann.append(time=crepe_time[i], value=value, duration=0, confidence=confidence[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-rap_env]",
   "language": "python",
   "name": "conda-env-.conda-rap_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
